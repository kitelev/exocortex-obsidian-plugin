name: 'Enhanced E2E Tests (Real Obsidian Desktop)'

on:
  push:
    branches: [main, develop]
    paths:
      - 'src/**'
      - 'tests/e2e/**'
      - 'package.json'
      - 'Dockerfile.e2e-enhanced'
      - 'docker-compose.e2e-enhanced.yml'
      - '.github/workflows/e2e-enhanced-tests.yml'
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/**'
      - 'tests/e2e/**'
      - 'package.json'
      - 'Dockerfile.e2e-enhanced'
      - 'docker-compose.e2e-enhanced.yml'
  schedule:
    # Run nightly for stability testing
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'full'
        type: choice
        options:
          - 'full'
          - 'smoke'
          - 'visual'
          - 'performance'
      obsidian_version:
        description: 'Obsidian version to test'
        required: false
        default: '1.5.12'
        type: string
      enable_debug:
        description: 'Enable VNC debugging'
        required: false
        default: false
        type: boolean
      parallel_tests:
        description: 'Run tests in parallel'
        required: false
        default: true
        type: boolean

env:
  NODE_VERSION: '20'
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1
  OBSIDIAN_VERSION: ${{ github.event.inputs.obsidian_version || '1.5.12' }}

jobs:
  # Build and validate plugin
  build-plugin:
    name: 'Build Plugin'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      build-success: ${{ steps.build.outputs.success }}
      plugin-version: ${{ steps.version.outputs.version }}
    
    steps:
      - name: 'Checkout Repository'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: 'Setup Node.js'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 'Install Dependencies'
        run: npm ci

      - name: 'Build Plugin'
        id: build
        run: |
          npm run build
          ls -la main.js manifest.json styles.css || true
          
          if [ -f main.js ] && [ -f manifest.json ]; then
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: 'Extract Plugin Version'
        id: version
        run: |
          VERSION=$(node -p "require('./manifest.json').version")
          echo "version=$VERSION" >> $GITHUB_OUTPUT

      - name: 'Upload Plugin Artifacts'
        uses: actions/upload-artifact@v4
        with:
          name: plugin-build-${{ github.run_number }}
          path: |
            main.js
            manifest.json
            styles.css
          retention-days: 7

  # Enhanced E2E Tests with Real Obsidian Desktop
  e2e-enhanced-tests:
    name: 'Enhanced E2E Tests'
    runs-on: ubuntu-latest
    needs: build-plugin
    if: needs.build-plugin.outputs.build-success == 'true'
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        test-profile: [
          { name: "full", compose-profile: "", timeout: 60 },
          { name: "visual", compose-profile: "", timeout: 45 },
        ]
        include:
          - test-profile: { name: "debug", compose-profile: "debug", timeout: 120 }
            if: ${{ github.event.inputs.enable_debug == 'true' }}
    
    steps:
      - name: 'Checkout Repository'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: 'Setup Node.js'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 'Download Plugin Build'
        uses: actions/download-artifact@v4
        with:
          name: plugin-build-${{ github.run_number }}
          path: .

      - name: 'Install Dependencies'
        run: |
          npm ci
          # Install additional testing dependencies
          npm install --save-dev \
            puppeteer@23.11.1 \
            playwright@1.45.0 \
            @playwright/test@1.45.0 \
            sharp@0.33.4

      - name: 'Setup Docker Buildx'
        uses: docker/setup-buildx-action@v3
        with:
          buildkitd-flags: --debug

      - name: 'Create Test Directories'
        run: |
          mkdir -p test-results/enhanced-screenshots
          mkdir -p test-results/videos
          mkdir -p test-results/reports
          mkdir -p test-results/allure-results
          mkdir -p playwright-report
          mkdir -p wdio-logs
          chmod -R 755 test-results playwright-report wdio-logs

      - name: 'Prepare Test Environment'
        run: |
          # Create enhanced test vault structure
          mkdir -p tests/e2e/test-vault/{assets,classes,templates,.obsidian/plugins/exocortex}
          
          # Copy plugin files
          cp main.js tests/e2e/test-vault/.obsidian/plugins/exocortex/
          cp manifest.json tests/e2e/test-vault/.obsidian/plugins/exocortex/
          cp styles.css tests/e2e/test-vault/.obsidian/plugins/exocortex/ || true
          
          # Create test assets
          echo "# Enhanced Test Asset
          exo__Class:: emo__Project
          exo__Name:: Enhanced E2E Test Project
          exo__Status:: exo__Active
          
          ## Test Query Block
          \`\`\`exo-query
          instances of exo__Project
          \`\`\`" > tests/e2e/test-vault/assets/Enhanced-Test-Asset.md
          
          # Enable plugin in Obsidian config
          echo '{"enabledPlugins":["exocortex"]}' > tests/e2e/test-vault/.obsidian/community-plugins.json

      - name: 'Build Enhanced E2E Docker Image'
        run: |
          docker build -f Dockerfile.e2e-enhanced -t exocortex-e2e-enhanced:latest \
            --build-arg NODE_VERSION=${{ env.NODE_VERSION }} \
            --build-arg OBSIDIAN_VERSION=${{ env.OBSIDIAN_VERSION }} \
            .

      - name: 'Run Enhanced E2E Tests'
        id: e2e-tests
        timeout-minutes: ${{ matrix.test-profile.timeout }}
        run: |
          set -e
          
          echo "🚀 Starting Enhanced E2E Tests with Real Obsidian Desktop"
          echo "============================================================"
          echo "Test Profile: ${{ matrix.test-profile.name }}"
          echo "Obsidian Version: ${{ env.OBSIDIAN_VERSION }}"
          echo "Docker Profile: ${{ matrix.test-profile.compose-profile }}"
          echo ""
          
          # Run the enhanced tests
          if [ "${{ matrix.test-profile.compose-profile }}" != "" ]; then
            docker-compose -f docker-compose.e2e-enhanced.yml --profile ${{ matrix.test-profile.compose-profile }} up --abort-on-container-exit
          else
            docker-compose -f docker-compose.e2e-enhanced.yml up e2e-enhanced-tests --abort-on-container-exit
          fi
          
          # Check if tests produced results
          if [ -f "test-results/reports/consolidated_report.json" ]; then
            echo "✅ Enhanced E2E tests completed with results"
            echo "test-success=true" >> $GITHUB_OUTPUT
          else
            echo "❌ No test results found"
            echo "test-success=false" >> $GITHUB_OUTPUT
          fi
        env:
          COMPOSE_DOCKER_CLI_BUILD: 1
          DOCKER_BUILDKIT: 1
          TEST_PROFILE: ${{ matrix.test-profile.name }}

      - name: 'Capture Container Logs'
        if: always()
        run: |
          echo "📋 Capturing container logs..."
          mkdir -p test-results/container-logs
          
          # Get all container logs
          docker-compose -f docker-compose.e2e-enhanced.yml logs > test-results/container-logs/all-services.log 2>&1 || true
          
          # Individual service logs
          docker-compose -f docker-compose.e2e-enhanced.yml logs e2e-enhanced-tests > test-results/container-logs/e2e-tests.log 2>&1 || true
          
          # Container status
          docker-compose -f docker-compose.e2e-enhanced.yml ps > test-results/container-logs/container-status.log 2>&1 || true
          
          # System information
          docker system df > test-results/container-logs/docker-system.log 2>&1 || true
          docker stats --no-stream > test-results/container-logs/docker-stats.log 2>&1 || true

      - name: 'Generate Visual Summary'
        if: always()
        run: |
          echo "🖼️  Generating visual test summary..."
          
          # Count artifacts
          SCREENSHOTS=$(find test-results/enhanced-screenshots -name "*.png" 2>/dev/null | wc -l)
          VIDEOS=$(find test-results/videos -name "*.mp4" -o -name "*.webm" 2>/dev/null | wc -l)
          REPORTS=$(find test-results/reports -name "*.json" -o -name "*.html" 2>/dev/null | wc -l)
          
          # Create summary
          cat > test-results/visual-summary.md << EOF
          # 🎯 Enhanced E2E Test Visual Summary
          
          **Test Profile:** ${{ matrix.test-profile.name }}  
          **Obsidian Version:** ${{ env.OBSIDIAN_VERSION }}  
          **Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          
          ## 📊 Artifacts Generated
          
          - **Screenshots:** ${SCREENSHOTS}
          - **Videos:** ${VIDEOS}  
          - **Reports:** ${REPORTS}
          
          ## 🔥 Real Plugin Testing Features
          
          ✅ **Actual Obsidian Desktop:** Running real Obsidian application in Docker with Xvfb  
          ✅ **Plugin Functionality:** Testing actual plugin features, not mocks or simulations  
          ✅ **Visual Testing:** Real screenshots showing actual plugin UI components  
          ✅ **Performance Monitoring:** Memory usage and execution time tracking  
          ✅ **Error Detection:** Real error catching from actual Obsidian environment  
          
          EOF
          
          echo "Visual summary created with ${SCREENSHOTS} screenshots, ${VIDEOS} videos, ${REPORTS} reports"

      - name: 'Upload Enhanced Test Results'
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: enhanced-e2e-results-${{ matrix.test-profile.name }}-${{ github.run_number }}
          path: |
            test-results/
            playwright-report/
            wdio-logs/
          retention-days: 30

      - name: 'Upload Screenshots'
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: enhanced-screenshots-${{ matrix.test-profile.name }}-${{ github.run_number }}
          path: test-results/enhanced-screenshots/
          retention-days: 14

      - name: 'Upload Container Logs'
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: container-logs-${{ matrix.test-profile.name }}-${{ github.run_number }}
          path: test-results/container-logs/
          retention-days: 7

      - name: 'Cleanup Docker Resources'
        if: always()
        run: |
          echo "🧹 Cleaning up Docker resources..."
          docker-compose -f docker-compose.e2e-enhanced.yml down --volumes --remove-orphans || true
          docker system prune -f || true

  # Performance Analysis Job
  performance-analysis:
    name: 'Performance Analysis'
    runs-on: ubuntu-latest
    needs: e2e-enhanced-tests
    if: always() && (github.event_name == 'schedule' || github.event.inputs.test_suite == 'performance')
    timeout-minutes: 30
    
    steps:
      - name: 'Checkout Repository'
        uses: actions/checkout@v4

      - name: 'Setup Node.js'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: 'Download All Test Results'
        uses: actions/download-artifact@v4
        with:
          pattern: enhanced-e2e-results-*-${{ github.run_number }}
          path: combined-results/
          merge-multiple: true

      - name: 'Analyze Performance Trends'
        run: |
          echo "📈 Analyzing performance trends..."
          
          # Create performance analysis script
          cat > analyze-performance.js << 'EOF'
          const fs = require('fs');
          const path = require('path');
          
          function analyzePerformance() {
              const resultsDir = 'combined-results/test-results/reports';
              if (!fs.existsSync(resultsDir)) {
                  console.log('No performance data found');
                  return;
              }
              
              const reportFiles = fs.readdirSync(resultsDir)
                  .filter(f => f.includes('consolidated_report.json'));
              
              const performanceData = [];
              
              reportFiles.forEach(file => {
                  try {
                      const data = JSON.parse(fs.readFileSync(path.join(resultsDir, file)));
                      if (data.performance && data.performance.averageTestDuration) {
                          performanceData.push({
                              timestamp: data.timestamp,
                              avgDuration: data.performance.averageTestDuration,
                              avgMemory: data.performance.averageMemoryUsage || 0,
                              successRate: data.summary.successRate
                          });
                      }
                  } catch (e) {
                      console.error('Error parsing', file, e.message);
                  }
              });
              
              if (performanceData.length > 0) {
                  const latest = performanceData[performanceData.length - 1];
                  console.log('📊 Performance Summary:');
                  console.log(`   Average Test Duration: ${latest.avgDuration}ms`);
                  console.log(`   Average Memory Usage: ${latest.avgMemory}MB`);
                  console.log(`   Success Rate: ${latest.successRate}%`);
                  
                  // Save performance summary
                  fs.writeFileSync('performance-summary.json', JSON.stringify({
                      summary: latest,
                      trend: performanceData,
                      analysis: {
                          stable: performanceData.every(p => p.successRate >= 80),
                          performanceRegression: performanceData.length > 1 && 
                              latest.avgDuration > performanceData[performanceData.length - 2].avgDuration * 1.2
                      }
                  }, null, 2));
              }
          }
          
          analyzePerformance();
          EOF
          
          node analyze-performance.js

      - name: 'Upload Performance Analysis'
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-analysis-${{ github.run_number }}
          path: |
            performance-summary.json
            analyze-performance.js
          retention-days: 30

  # Report Results and Summary
  test-summary:
    name: 'Test Summary'
    runs-on: ubuntu-latest
    needs: [build-plugin, e2e-enhanced-tests]
    if: always()
    timeout-minutes: 15
    
    steps:
      - name: 'Download All Test Results'
        uses: actions/download-artifact@v4
        with:
          pattern: enhanced-e2e-results-*-${{ github.run_number }}
          path: all-results/
          merge-multiple: true

      - name: 'Generate Consolidated Summary'
        id: summary
        run: |
          echo "📋 Generating consolidated test summary..."
          
          # Count results
          TOTAL_SCREENSHOTS=$(find all-results -name "*.png" 2>/dev/null | wc -l || echo 0)
          TOTAL_REPORTS=$(find all-results -name "*.json" 2>/dev/null | wc -l || echo 0)
          TOTAL_HTML_REPORTS=$(find all-results -name "*.html" 2>/dev/null | wc -l || echo 0)
          
          # Check for consolidated reports
          SUCCESS_COUNT=0
          FAILURE_COUNT=0
          
          for report in $(find all-results -name "*consolidated_report.json" 2>/dev/null); do
              if [ -f "$report" ]; then
                  PASSED=$(jq -r '.summary.passed // 0' "$report")
                  FAILED=$(jq -r '.summary.failed // 0' "$report")
                  SUCCESS_COUNT=$((SUCCESS_COUNT + PASSED))
                  FAILURE_COUNT=$((FAILURE_COUNT + FAILED))
              fi
          done
          
          TOTAL_TESTS=$((SUCCESS_COUNT + FAILURE_COUNT))
          if [ $TOTAL_TESTS -gt 0 ]; then
              SUCCESS_RATE=$(echo "scale=1; $SUCCESS_COUNT * 100 / $TOTAL_TESTS" | bc -l 2>/dev/null || echo "0")
          else
              SUCCESS_RATE="0"
          fi
          
          # Create summary
          cat > test-summary.md << EOF
          # 🎯 Enhanced E2E Test Results Summary
          
          **Plugin Version:** ${{ needs.build-plugin.outputs.plugin-version }}  
          **Obsidian Version:** ${{ env.OBSIDIAN_VERSION }}  
          **Node Version:** ${{ env.NODE_VERSION }}  
          **Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          
          ## 📊 Test Results
          
          | Metric | Value |
          |--------|-------|
          | Total Tests | ${TOTAL_TESTS} |
          | Passed | ✅ ${SUCCESS_COUNT} |
          | Failed | ❌ ${FAILURE_COUNT} |
          | Success Rate | ${SUCCESS_RATE}% |
          | Screenshots | 📸 ${TOTAL_SCREENSHOTS} |
          | JSON Reports | 📄 ${TOTAL_REPORTS} |
          | HTML Reports | 🌐 ${TOTAL_HTML_REPORTS} |
          
          ## 🔥 Enhanced Testing Features
          
          ✅ **Real Obsidian Desktop** - Actual Obsidian application running in Docker with Xvfb  
          ✅ **Authentic Plugin Testing** - No mocks, no simulations, 100% real plugin functionality  
          ✅ **Visual Regression Testing** - Real screenshots showing actual plugin UI components  
          ✅ **Performance Monitoring** - Memory usage, execution time, and resource tracking  
          ✅ **Comprehensive Coverage** - UI components, commands, settings, error handling  
          ✅ **Multi-Test Runner Support** - Puppeteer, Playwright, and WebDriverIO integration  
          
          ## 🎯 Test Execution
          
          All tests run against **real Obsidian desktop application** in containerized environment with:
          - Xvfb virtual display server for headless operation
          - Full plugin loading and initialization
          - Actual DOM manipulation and interaction
          - Real error detection and performance monitoring
          
          EOF
          
          echo "summary-path=test-summary.md" >> $GITHUB_OUTPUT
          echo "total-tests=${TOTAL_TESTS}" >> $GITHUB_OUTPUT
          echo "success-rate=${SUCCESS_RATE}" >> $GITHUB_OUTPUT
          echo "screenshots=${TOTAL_SCREENSHOTS}" >> $GITHUB_OUTPUT
          
          cat test-summary.md

      - name: 'Comment PR with Results'
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('${{ steps.summary.outputs.summary-path }}')) {
              const summary = fs.readFileSync('${{ steps.summary.outputs.summary-path }}', 'utf8');
              
              // Add status emoji based on results
              const successRate = parseFloat('${{ steps.summary.outputs.success-rate }}');
              const statusEmoji = successRate >= 90 ? '🎉' : successRate >= 70 ? '⚠️' : '💥';
              const statusText = successRate >= 90 ? 'EXCELLENT' : successRate >= 70 ? 'GOOD' : 'NEEDS ATTENTION';
              
              const enhancedSummary = `${statusEmoji} **Enhanced E2E Test Results: ${statusText}**\n\n${summary}`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: enhancedSummary
              });
            }

      - name: 'Upload Test Summary'
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-summary-${{ github.run_number }}
          path: test-summary.md
          retention-days: 30

      - name: 'Set Job Status'
        run: |
          SUCCESS_RATE=$(echo "${{ steps.summary.outputs.success-rate }}" | cut -d'.' -f1)
          if [ "$SUCCESS_RATE" -ge 80 ]; then
            echo "✅ Enhanced E2E tests completed successfully with ${SUCCESS_RATE}% success rate"
            exit 0
          else
            echo "❌ Enhanced E2E tests completed with low success rate: ${SUCCESS_RATE}%"
            exit 1
          fi